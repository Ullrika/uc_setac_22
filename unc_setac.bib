@article{benfordPrinciplesMethodsEFSA2018,
  title = {The Principles and Methods behind {{EFSA}}'s Guidance on Uncertainty Analysis in Scientific Assessment},
  author = {Benford, Diane and Halldorsson, Thorhallur and Jeger, Michael John and Knutsen, Helle Katrine and More, Simon and Naegeli, Hanspeter and Noteborn, Hubert and Ockleford, Colin and Ricci, Antonia and Rychen, Guido and Schlatter, Josef R and Silano, Vittorio and Solecki, Roland and Turck, Dominique and Younes, Maged and Craig, Peter and Hart, Andrew and Von Goetz, Natalie and Koutsoumanis, Kostas and Mortensen, Alicja and Ossendorp, Bernadette and Germini, Andrea and Martino, Laura and Merten, Caroline and {Mosbach-Schulz}, Olaf and Smith, Anthony and Hardy, Anthony},
  year = {2018},
  journal = {EFSA Journal},
  volume = {16},
  number = {1},
  pages = {e05122},
  doi = {10.2903/j.efsa.2018.5122},
  abstract = {Abstract To meet the general requirement for transparency in EFSA's work, all its scientific assessments must consider uncertainty. Assessments must say clearly and unambiguously what sources of uncertainty have been identified and what is their impact on the assessment conclusion. This applies to all EFSA's areas, all types of scientific assessment and all types of uncertainty affecting assessment. This current Opinion describes the principles and methods supporting a concise Guidance Document on Uncertainty in EFSA's Scientific Assessment, published separately. These documents do not prescribe specific methods for uncertainty analysis but rather provide a flexible framework within which different methods may be selected, according to the needs of each assessment. Assessors should systematically identify sources of uncertainty, checking each part of their assessment to minimise the risk of overlooking important uncertainties. Uncertainty may be expressed qualitatively or quantitatively. It is neither necessary nor possible to quantify separately every source of uncertainty affecting an assessment. However, assessors should express in quantitative terms the combined effect of as many as possible of identified sources of uncertainty. The guidance describes practical approaches. Uncertainty analysis should be conducted in a flexible, iterative manner, starting at a level appropriate to the assessment and refining the analysis as far as is needed or possible within the time available. The methods and results of the uncertainty analysis should be reported fully and transparently. Every EFSA Panel and Unit applied the draft Guidance to at least one assessment in their work area during a trial period of one year. Experience gained in this period resulted in improved guidance. The Scientific Committee considers that uncertainty analysis will be unconditional for EFSA Panels and staff and must be embedded into scientific assessment in all areas of EFSA's work.},
  keywords = {guidance,principles,scientific assessment,uncertainty analysis},
  file = {C\:\\Users\\ekol-usa\\ArticlesBooks\\PrinciplesUncertaintyEFSA2018.pdf}
}

@incollection{Ramsey_1926,
  title = {Truth and Probability},
  author = {Ramsey, Frank},
  year = {2016},
  month = jan,
  pages = {21--45},
  doi = {10.1007/978-3-319-20451-2_3},
  isbn = {978-3-319-20450-5}
}



@article{efsaGuidanceCommunicationUncertainty2019,
  title = {Guidance on Communication of Uncertainty in Scientific Assessments},
  author = {EFSA},
  year = {2019},
  journal = {EFSA Journal},
  volume = {17},
  number = {1},
  pages = {e05520},
  file = {C\:\\Users\\ekol-usa\\ArticlesBooks\\CommunicationUncertaintyEFSA2019.pdf}
}

@article{langendamAssessingPresentingSummaries2013,
  title = {Assessing and Presenting Summaries of Evidence in {{Cochrane Reviews}}},
  author = {Langendam, Miranda W. and Akl, Elie A. and Dahm, Philipp and Glasziou, Paul and Guyatt, Gordon and Sch{\"u}nemann, Holger J.},
  year = {2013},
  month = sep,
  journal = {Systematic Reviews},
  volume = {2},
  number = {1},
  pages = {81},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-2-81},
  abstract = {Cochrane Reviews are intended to help providers, practitioners and patients make informed decisions about health care. The goal of the Cochrane Applicability and Recommendation Methods Group (ARMG) is to develop approaches, strategies and guidance that facilitate the uptake of information from Cochrane Reviews and their use by a wide audience with specific focus on developers of recommendations and on healthcare decision makers. This paper is part of a series highlighting developments in systematic review methodology in the 20 years since the establishment of The Cochrane Collaboration, and its aim is to present current work and highlight future developments in assessing and presenting summaries of evidence, with special focus on Summary of Findings (SoF) tables and Plain Language Summaries.}
}

@article{mastrandreaIPCCAR5Guidance2011,
  title = {The {{IPCC AR5}} Guidance Note on Consistent Treatment of Uncertainties: A Common Approach across the Working Groups},
  shorttitle = {The {{IPCC AR5}} Guidance Note on Consistent Treatment of Uncertainties},
  author = {Mastrandrea, Michael D. and Mach, Katharine J. and Plattner, Gian-Kasper and Edenhofer, Ottmar and Stocker, Thomas F. and Field, Christopher B. and Ebi, Kristie L. and Matschoss, Patrick R.},
  year = {2011},
  month = oct,
  journal = {Climatic Change},
  volume = {108},
  number = {4},
  pages = {675--691},
  issn = {0165-0009, 1573-1480},
  doi = {10.1007/s10584-011-0178-6},
  langid = {english}
}

@article{morganGRADEAssessingQuality2016,
  title = {{{GRADE}}: {{Assessing}} the Quality of Evidence in Environmental and Occupational Health},
  shorttitle = {{{GRADE}}},
  author = {Morgan, Rebecca L. and Thayer, Kristina A. and Bero, Lisa and Bruce, Nigel and {Falck-Ytter}, Yngve and Ghersi, Davina and Guyatt, Gordon and Hooijmans, Carlijn and Langendam, Miranda and Mandrioli, Daniele and Mustafa, Reem A. and Rehfuess, Eva A. and Rooney, Andrew A. and Shea, Beverley and Silbergeld, Ellen K. and Sutton, Patrice and Wolfe, Mary S. and Woodruff, Tracey J. and Verbeek, Jos H. and Holloway, Alison C. and Santesso, Nancy and Sch{\"u}nemann, Holger J.},
  year = {2016},
  month = jul,
  journal = {Environment International},
  volume = {92--93},
  pages = {611--616},
  issn = {01604120},
  doi = {10.1016/j.envint.2016.01.004},
  langid = {english}
}

@article{padillaUncertainUncertaintyHow2021,
  title = {Uncertain {{About Uncertainty}}: {{How Qualitative Expressions}} of {{Forecaster Confidence Impact Decision-Making With Uncertainty Visualizations}}},
  author = {Padilla, Lace M. K. and Powell, Maia and Kay, Matthew and Hullman, Jessica},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {11},
  pages = {3747},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.579267},
  abstract = {When forecasting events, multiple types of uncertainty are often inherently present in the modeling process. Various uncertainty typologies exist, and each type of uncertainty has different implications a scientist might want to convey. In this work, we focus on one type of distinction between direct quantitative uncertainty and indirect qualitative uncertainty. Direct quantitative uncertainty describes uncertainty about facts, numbers, and hypotheses that can be communicated in absolute quantitative forms such as probability distributions or confidence intervals. Indirect qualitative uncertainty describes the quality of knowledge concerning how effectively facts, numbers, or hypotheses represent reality, such as evidence confidence scales proposed by the Intergovernmental Panel on Climate Change. A large body of research demonstrates that both experts and novices have difficulty reasoning with quantitative uncertainty, and visualizations of uncertainty can help with such traditionally challenging concepts. However, the question of if, and how, people may reason with multiple types of uncertainty associated with a forecast remains largely unexplored. In this series of studies, we seek to understand if individuals can integrate indirect uncertainty about how ``good'' a model is (operationalized as a qualitative expression of forecaster confidence) with quantified uncertainty in a prediction (operationalized as a quantile dotplot visualization of a predicted distribution). Our first study results suggest that participants utilize both direct quantitative uncertainty and indirect qualitative uncertainty when conveyed as quantile dotplots and forecaster confidence. In manipulations where forecasters were less sure about their prediction, participants made more conservative judgments. In our second study, we varied the amount of quantified uncertainty (in the form of the SD of the visualized distributions) to examine how participants' decisions changed under different combinations of quantified uncertainty (variance) and qualitative uncertainty (low, medium, and high forecaster confidence). The second study results suggest that participants updated their judgments in the direction predicted by both qualitative confidence information (e.g., becoming more conservative when the forecaster confidence is low) and quantitative uncertainty (e.g., becoming more conservative when the variance is increased). Based on the findings from both experiments, we recommend that forecasters present qualitative expressions of model confidence whenever possible alongside quantified uncertainty.},
  file = {C\:\\Users\\ekol-usa\\ArticlesBooks\\PadillaUncertainaboutuncertainty.pdf}
}

@article{SAPEA_2019,
  title = {Making Sense of Science for Policy under Conditions of Complexity and Uncertainty},
  author = {{SAPEA}},
  year = {2019},
  doi = {10.26356/MASOS}
}

@article{vanderblesCommunicatingUncertaintyFacts2019,
  ids = {vanderbles2019CommunicatingUncertaintyFactsa},
  title = {Communicating Uncertainty about Facts, Numbers and Science},
  author = {{van der Bles}, Anne Marthe and {van der Linden}, Sander and Freeman, Alexandra L. J. and Mitchell, James and Galvao, Ana B. and Zaval, Lisa and Spiegelhalter, David J.},
  year = {2019},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {6},
  number = {5},
  pages = {181870},
  doi = {10/gf2g9j},
  abstract = {Uncertainty is an inherent part of knowledge, and yet in an era of contested expertise, many shy away from openly communicating their uncertainty about what they know, fearful of their audience's reaction. But what effect does communication of such epistemic uncertainty have? Empirical research is widely scattered across many disciplines. This interdisciplinary review structures and summarizes current practice and research across domains, combining a statistical and psychological perspective. This informs a framework for uncertainty communication in which we identify three objects of uncertainty\textemdash facts, numbers and science\textemdash and two levels of uncertainty: direct and indirect. An examination of current practices provides a scale of nine expressions of direct uncertainty. We discuss attempts to codify indirect uncertainty in terms of quality of the underlying evidence. We review the limited literature about the effects of communicating epistemic uncertainty on cognition, affect, trust and decision-making. While there is some evidence that communicating epistemic uncertainty does not necessarily affect audiences negatively, impact can vary between individuals and communication formats. Case studies in economic statistics and climate change illustrate our framework in action. We conclude with advice to guide both communicators and future researchers in this important but so far rather neglected field.}
}

